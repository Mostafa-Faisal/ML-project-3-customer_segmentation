{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d64ff0",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "This notebook is optimized to run in Google Colab. Click the button below to open it directly in Colab:\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/your-notebook.ipynb)\n",
    "\n",
    "## Quick Start Instructions for Google Colab:\n",
    "1. Click \"Runtime\" ‚Üí \"Run all\" to execute all cells\n",
    "2. Or run cells individually using Shift+Enter\n",
    "3. All required packages will be installed automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51846b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "# This cell will install any missing packages\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install package if not already installed\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "\n",
    "# Check and install required packages\n",
    "required_packages = [\n",
    "    'pandas',\n",
    "    'numpy', \n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'joblib'\n",
    "]\n",
    "\n",
    "print(\"üîç Checking required packages...\")\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nüéâ All packages are ready!\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"\\nüêç Python version: {sys.version}\")\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"üíª Running in local environment\")\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ad68d",
   "metadata": {},
   "source": [
    "## üìÅ Upload Your Dataset\n",
    "\n",
    "### Option 1: Use Your Own Online Retail.xlsx File\n",
    "If you have the **Online Retail.xlsx** dataset:\n",
    "\n",
    "**In Google Colab:**\n",
    "1. Run the upload cell below\n",
    "2. Click \"Choose Files\" when prompted\n",
    "3. Select your `Online Retail.xlsx` file\n",
    "4. The notebook will automatically process it for customer segmentation\n",
    "\n",
    "**In Local Environment:**\n",
    "1. Place the `Online Retail.xlsx` file in the same directory as this notebook\n",
    "2. The notebook will automatically detect and load it\n",
    "\n",
    "### Option 2: Use Synthetic Data\n",
    "If you don't have the dataset, the notebook will automatically generate realistic synthetic customer data for demonstration purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4e71b",
   "metadata": {},
   "source": [
    "# Task 2: Retail Customer Segmentation using K-Means Clustering\n",
    "\n",
    "## Project Overview\n",
    "This project implements a K-Means clustering algorithm to group customers based on their purchase history and spending behavior. We'll use the following techniques:\n",
    "\n",
    "- **K-Means Clustering** for customer segmentation\n",
    "- **Elbow Method** to determine the optimal number of clusters\n",
    "- **PCA** for data visualization\n",
    "- **Data preprocessing** and feature engineering\n",
    "\n",
    "## Dataset\n",
    "We'll use the Customer Segmentation Dataset from Kaggle that contains customer purchase history and demographic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45423c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "# Use Colab-compatible matplotlib style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    # Fallback for older matplotlib versions in Colab\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib for Colab\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a25da",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload your own dataset (Online Retail.xlsx)\n",
    "# This cell allows you to upload the Online Retail.xlsx file\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Check if running in Google Colab for file upload\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"üìÅ Upload your Online Retail.xlsx file:\")\n",
    "    print(\"Click 'Choose Files' and select your Online Retail.xlsx file\")\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Process uploaded file\n",
    "    if uploaded:\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        print(f\"‚úÖ File uploaded: {filename}\")\n",
    "        \n",
    "        # Read the Excel file\n",
    "        if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
    "            df_raw = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
    "            print(f\"üìä Dataset loaded successfully!\")\n",
    "            print(f\"Shape: {df_raw.shape}\")\n",
    "            print(f\"Columns: {list(df_raw.columns)}\")\n",
    "            \n",
    "            # Display first few rows\n",
    "            print(\"\\nFirst 5 rows of your dataset:\")\n",
    "            print(df_raw.head())\n",
    "            \n",
    "            USE_UPLOADED_DATA = True\n",
    "        else:\n",
    "            print(\"‚ùå Please upload an Excel file (.xlsx or .xls)\")\n",
    "            USE_UPLOADED_DATA = False\n",
    "    else:\n",
    "        print(\"‚ùå No file uploaded. Will use synthetic data instead.\")\n",
    "        USE_UPLOADED_DATA = False\n",
    "else:\n",
    "    # For local environment, try to read the file if it exists\n",
    "    try:\n",
    "        df_raw = pd.read_excel('Online Retail.xlsx')\n",
    "        print(\"‚úÖ Online Retail.xlsx found and loaded!\")\n",
    "        print(f\"Shape: {df_raw.shape}\")\n",
    "        print(f\"Columns: {list(df_raw.columns)}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df_raw.head())\n",
    "        USE_UPLOADED_DATA = True\n",
    "    except FileNotFoundError:\n",
    "        print(\"üìÅ Online Retail.xlsx not found in current directory.\")\n",
    "        print(\"üí° Please place the file in the same directory as this notebook, or use the upload option in Colab.\")\n",
    "        USE_UPLOADED_DATA = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        USE_UPLOADED_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72726ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Online Retail Dataset for Customer Segmentation\n",
    "if USE_UPLOADED_DATA:\n",
    "    print(\"üîÑ Processing Online Retail dataset for customer segmentation...\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"Shape: {df_raw.shape}\")\n",
    "    print(f\"Columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    # Common column names in Online Retail datasets\n",
    "    # Try to identify the correct column names\n",
    "    columns_mapping = {}\n",
    "    \n",
    "    # Look for common patterns in column names\n",
    "    for col in df_raw.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        if 'customer' in col_lower or 'customerid' in col_lower:\n",
    "            columns_mapping['CustomerID'] = col\n",
    "        elif 'invoice' in col_lower and ('no' in col_lower or 'number' in col_lower):\n",
    "            columns_mapping['InvoiceNo'] = col\n",
    "        elif 'invoice' in col_lower and 'date' in col_lower:\n",
    "            columns_mapping['InvoiceDate'] = col\n",
    "        elif 'stock' in col_lower or 'product' in col_lower:\n",
    "            columns_mapping['StockCode'] = col\n",
    "        elif 'description' in col_lower:\n",
    "            columns_mapping['Description'] = col\n",
    "        elif 'quantity' in col_lower:\n",
    "            columns_mapping['Quantity'] = col\n",
    "        elif 'price' in col_lower or 'unit' in col_lower:\n",
    "            columns_mapping['UnitPrice'] = col\n",
    "        elif 'country' in col_lower:\n",
    "            columns_mapping['Country'] = col\n",
    "    \n",
    "    print(f\"\\nIdentified columns: {columns_mapping}\")\n",
    "    \n",
    "    # Create a copy with standardized column names\n",
    "    df_retail = df_raw.copy()\n",
    "    df_retail = df_retail.rename(columns=columns_mapping)\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    print(\"\\nüßπ Cleaning data...\")\n",
    "    \n",
    "    # Remove missing customer IDs\n",
    "    initial_rows = len(df_retail)\n",
    "    df_retail = df_retail.dropna(subset=['CustomerID'])\n",
    "    print(f\"Removed {initial_rows - len(df_retail)} rows with missing CustomerID\")\n",
    "    \n",
    "    # Remove negative quantities and prices (returns)\n",
    "    df_retail = df_retail[df_retail['Quantity'] > 0]\n",
    "    df_retail = df_retail[df_retail['UnitPrice'] > 0]\n",
    "    \n",
    "    # Calculate total amount per transaction\n",
    "    df_retail['TotalAmount'] = df_retail['Quantity'] * df_retail['UnitPrice']\n",
    "    \n",
    "    # Convert InvoiceDate to datetime\n",
    "    df_retail['InvoiceDate'] = pd.to_datetime(df_retail['InvoiceDate'])\n",
    "    \n",
    "    print(f\"Final dataset shape: {df_retail.shape}\")\n",
    "    print(\"\\nProcessed dataset sample:\")\n",
    "    print(df_retail[['CustomerID', 'InvoiceDate', 'Quantity', 'UnitPrice', 'TotalAmount']].head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No valid dataset uploaded. Will use synthetic data for demonstration.\")\n",
    "    df_retail = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Customer Features using RFM Analysis\n",
    "if USE_UPLOADED_DATA and df_retail is not None:\n",
    "    print(\"üìä Creating customer features using RFM (Recency, Frequency, Monetary) analysis...\")\n",
    "    \n",
    "    # Get the latest date in the dataset\n",
    "    latest_date = df_retail['InvoiceDate'].max()\n",
    "    print(f\"Analysis date: {latest_date}\")\n",
    "    \n",
    "    # Create RFM features for each customer\n",
    "    customer_features = df_retail.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (latest_date - x.max()).days,  # Recency\n",
    "        'InvoiceNo': 'nunique',  # Frequency (number of unique invoices)\n",
    "        'TotalAmount': ['sum', 'mean', 'count'],  # Monetary\n",
    "        'Quantity': ['sum', 'mean']  # Additional features\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    customer_features.columns = [\n",
    "        'Recency_Days',\n",
    "        'Purchase_Frequency', \n",
    "        'Total_Spent',\n",
    "        'Average_Order_Value',\n",
    "        'Total_Items_Purchased',\n",
    "        'Total_Quantity',\n",
    "        'Average_Quantity_Per_Order'\n",
    "    ]\n",
    "    \n",
    "    # Reset index to make CustomerID a column\n",
    "    customer_features = customer_features.reset_index()\n",
    "    \n",
    "    # Create additional features\n",
    "    customer_features['Days_Since_First_Purchase'] = df_retail.groupby('CustomerID')['InvoiceDate'].agg(\n",
    "        lambda x: (latest_date - x.min()).days\n",
    "    ).values\n",
    "    \n",
    "    customer_features['Customer_Lifetime_Days'] = (\n",
    "        customer_features['Days_Since_First_Purchase'] - customer_features['Recency_Days']\n",
    "    )\n",
    "    \n",
    "    # Calculate purchase rate (frequency per day)\n",
    "    customer_features['Purchase_Rate'] = (\n",
    "        customer_features['Purchase_Frequency'] / \n",
    "        (customer_features['Customer_Lifetime_Days'] + 1)  # +1 to avoid division by zero\n",
    "    ).round(4)\n",
    "    \n",
    "    # Create customer age groups based on first purchase\n",
    "    def categorize_customer_age():\n",
    "        # Since we don't have actual age, we'll create categories based on customer tenure\n",
    "        tenure_days = customer_features['Days_Since_First_Purchase']\n",
    "        return pd.cut(tenure_days, \n",
    "                     bins=[0, 30, 90, 180, 365, float('inf')],\n",
    "                     labels=['New (0-30d)', 'Recent (1-3m)', 'Regular (3-6m)', 'Loyal (6-12m)', 'VIP (1y+)'])\n",
    "    \n",
    "    customer_features['Customer_Segment_Tenure'] = categorize_customer_age()\n",
    "    \n",
    "    # Remove any customers with invalid data\n",
    "    customer_features = customer_features.dropna()\n",
    "    customer_features = customer_features[customer_features['Total_Spent'] > 0]\n",
    "    \n",
    "    print(f\"‚úÖ Customer features created for {len(customer_features)} customers\")\n",
    "    print(f\"\\nFeature columns: {list(customer_features.columns)}\")\n",
    "    print(f\"\\nCustomer features summary:\")\n",
    "    print(customer_features.describe())\n",
    "    \n",
    "    # Display sample customers\n",
    "    print(f\"\\nSample customer features:\")\n",
    "    print(customer_features.head(10))\n",
    "    \n",
    "    # Use processed retail data\n",
    "    df = customer_features.copy()\n",
    "    \n",
    "    # Rename columns to match the original synthetic data structure\n",
    "    df = df.rename(columns={\n",
    "        'Total_Spent': 'Annual_Income',  # Using total spent as a proxy\n",
    "        'Purchase_Frequency': 'Purchase_Frequency',\n",
    "        'Average_Order_Value': 'Average_Order_Value',\n",
    "        'Recency_Days': 'Days_Since_Last_Purchase'\n",
    "    })\n",
    "    \n",
    "    # Create synthetic age and years_customer based on tenure\n",
    "    df['Age'] = np.random.normal(40, 12, len(df)).astype(int)\n",
    "    df['Age'] = np.clip(df['Age'], 18, 80)\n",
    "    \n",
    "    # Use customer lifetime as years customer (convert days to years)\n",
    "    df['Years_Customer'] = (df['Customer_Lifetime_Days'] / 365).round(1)\n",
    "    df['Years_Customer'] = np.clip(df['Years_Customer'], 0.1, 10)\n",
    "    \n",
    "    # Create spending score based on total spent (normalize to 1-100)\n",
    "    spending_scores = df['Annual_Income'].rank(pct=True) * 100\n",
    "    df['Spending_Score'] = spending_scores.round(0).astype(int)\n",
    "    \n",
    "    print(f\"\\nüéØ Final dataset prepared for clustering:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using synthetic data for demonstration since no retail dataset was provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57486850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Create sample customer data (fallback if no real dataset is uploaded)\n",
    "if not USE_UPLOADED_DATA:\n",
    "    print(\"üìù Creating synthetic customer data for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_customers = 1000\n",
    "\n",
    "    # Generate synthetic customer data\n",
    "    data = {\n",
    "        'CustomerID': range(1, n_customers + 1),\n",
    "        'Age': np.random.normal(40, 12, n_customers).astype(int),\n",
    "        'Annual_Income': np.random.normal(60000, 20000, n_customers),\n",
    "        'Spending_Score': np.random.randint(1, 101, n_customers),\n",
    "        'Years_Customer': np.random.randint(1, 11, n_customers),\n",
    "        'Purchase_Frequency': np.random.poisson(15, n_customers),\n",
    "        'Average_Order_Value': np.random.normal(150, 50, n_customers)\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Ensure realistic ranges\n",
    "    df['Age'] = np.clip(df['Age'], 18, 80)\n",
    "    df['Annual_Income'] = np.clip(df['Annual_Income'], 20000, 150000)\n",
    "    df['Average_Order_Value'] = np.clip(df['Average_Order_Value'], 20, 500)\n",
    "\n",
    "    print(\"‚úÖ Synthetic dataset created successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"‚úÖ Using uploaded Online Retail dataset for analysis!\")\n",
    "\n",
    "# Final data summary\n",
    "print(f\"\\nüìä Final dataset for clustering:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Data source: {'Real Online Retail data' if USE_UPLOADED_DATA else 'Synthetic data'}\")\n",
    "print(f\"Number of customers: {len(df):,}\")\n",
    "\n",
    "# Show data types and basic info\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e847726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nDataset Description:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization for EDA\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Exploratory Data Analysis - Customer Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df['Age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Annual Income distribution\n",
    "axes[0, 1].hist(df['Annual_Income'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Annual Income Distribution')\n",
    "axes[0, 1].set_xlabel('Annual Income ($)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Spending Score distribution\n",
    "axes[0, 2].hist(df['Spending_Score'], bins=20, alpha=0.7, color='salmon', edgecolor='black')\n",
    "axes[0, 2].set_title('Spending Score Distribution')\n",
    "axes[0, 2].set_xlabel('Spending Score (1-100)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Purchase Frequency distribution\n",
    "axes[1, 0].hist(df['Purchase_Frequency'], bins=20, alpha=0.7, color='gold', edgecolor='black')\n",
    "axes[1, 0].set_title('Purchase Frequency Distribution')\n",
    "axes[1, 0].set_xlabel('Purchase Frequency')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Average Order Value distribution\n",
    "axes[1, 1].hist(df['Average_Order_Value'], bins=20, alpha=0.7, color='plum', edgecolor='black')\n",
    "axes[1, 1].set_title('Average Order Value Distribution')\n",
    "axes[1, 1].set_xlabel('Average Order Value ($)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Years as Customer distribution\n",
    "axes[1, 2].hist(df['Years_Customer'], bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1, 2].set_title('Years as Customer Distribution')\n",
    "axes[1, 2].set_xlabel('Years as Customer')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix of Customer Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key correlations observed:\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.3:  # Show only significant correlations\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14378500",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40866ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection for Clustering\n",
    "# Select relevant features for customer segmentation\n",
    "features_for_clustering = ['Age', 'Annual_Income', 'Spending_Score', \n",
    "                          'Purchase_Frequency', 'Average_Order_Value', 'Years_Customer']\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[features_for_clustering].copy()\n",
    "\n",
    "print(\"Features selected for clustering:\")\n",
    "print(X.columns.tolist())\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(\"\\nFeature statistics before scaling:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b57708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Standardize features to have mean=0 and std=1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_clustering)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(\"\\nFeature statistics after scaling:\")\n",
    "print(X_scaled_df.describe())\n",
    "\n",
    "# Visualize the effect of scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Before scaling\n",
    "axes[0].boxplot([X[col] for col in X.columns], labels=X.columns)\n",
    "axes[0].set_title('Features Before Scaling')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# After scaling\n",
    "axes[1].boxplot([X_scaled_df[col] for col in X_scaled_df.columns], labels=X_scaled_df.columns)\n",
    "axes[1].set_title('Features After Scaling')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4845438",
   "metadata": {},
   "source": [
    "## Step 3: Determine the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method to determine optimal number of clusters\n",
    "def elbow_method(X, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Implement elbow method to find optimal number of clusters\n",
    "    \"\"\"\n",
    "    wcss = []  # Within-cluster sum of squares\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    K_range = range(2, max_clusters + 1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        # Fit K-means\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, init='k-means++', n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Calculate WCSS\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calculate Silhouette Score\n",
    "        silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "        print(f\"k={k}: WCSS={kmeans.inertia_:.2f}, Silhouette Score={silhouette_avg:.3f}\")\n",
    "    \n",
    "    return K_range, wcss, silhouette_scores\n",
    "\n",
    "# Apply elbow method\n",
    "print(\"Applying Elbow Method...\")\n",
    "K_range, wcss, silhouette_scores = elbow_method(X_scaled, max_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5280246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Elbow Method and Silhouette Scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(K_range, wcss, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "for i, (k, w) in enumerate(zip(K_range, wcss)):\n",
    "    if k in [3, 4, 5]:  # Highlight potential optimal values\n",
    "        axes[0].annotate(f'k={k}', (k, w), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center', fontweight='bold')\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Silhouette Score Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the best silhouette score\n",
    "best_k = K_range[np.argmax(silhouette_scores)]\n",
    "best_score = max(silhouette_scores)\n",
    "axes[1].annotate(f'Best: k={best_k}\\nScore={best_score:.3f}', \n",
    "                (best_k, best_score), textcoords=\"offset points\", \n",
    "                xytext=(20,20), ha='left', fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRecommended number of clusters based on Silhouette Score: {best_k}\")\n",
    "print(f\"Best Silhouette Score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a712944",
   "metadata": {},
   "source": [
    "## Step 4: Apply K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40080866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal number of clusters\n",
    "optimal_k = best_k  # Use the best k from silhouette analysis\n",
    "\n",
    "print(f\"Applying K-Means clustering with k={optimal_k}...\")\n",
    "\n",
    "# Initialize and fit K-Means\n",
    "kmeans_optimal = KMeans(\n",
    "    n_clusters=optimal_k, \n",
    "    random_state=42, \n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "# Fit the model and predict clusters\n",
    "cluster_labels = kmeans_optimal.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"Clustering completed!\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n",
    "print(f\"Inertia (WCSS): {kmeans_optimal.inertia_:.2f}\")\n",
    "\n",
    "# Display cluster distribution\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Display first few rows with cluster assignments\n",
    "print(f\"\\nFirst 10 customers with their cluster assignments:\")\n",
    "print(df[['CustomerID', 'Age', 'Annual_Income', 'Spending_Score', 'Cluster']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ea9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "print(\"Cluster Characteristics Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cluster_summary = df.groupby('Cluster')[features_for_clustering].agg(['mean', 'std']).round(2)\n",
    "print(cluster_summary)\n",
    "\n",
    "# Create cluster profiles\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cluster_id in sorted(df['Cluster'].unique()):\n",
    "    cluster_data = df[df['Cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\nüìä CLUSTER {cluster_id} ({len(cluster_data)} customers, {len(cluster_data)/len(df)*100:.1f}%)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate means for this cluster\n",
    "    avg_age = cluster_data['Age'].mean()\n",
    "    avg_income = cluster_data['Annual_Income'].mean()\n",
    "    avg_spending = cluster_data['Spending_Score'].mean()\n",
    "    avg_frequency = cluster_data['Purchase_Frequency'].mean()\n",
    "    avg_order_value = cluster_data['Average_Order_Value'].mean()\n",
    "    avg_years = cluster_data['Years_Customer'].mean()\n",
    "    \n",
    "    print(f\"Average Age: {avg_age:.1f} years\")\n",
    "    print(f\"Average Income: ${avg_income:,.0f}\")\n",
    "    print(f\"Average Spending Score: {avg_spending:.1f}/100\")\n",
    "    print(f\"Average Purchase Frequency: {avg_frequency:.1f} purchases\")\n",
    "    print(f\"Average Order Value: ${avg_order_value:.2f}\")\n",
    "    print(f\"Average Years as Customer: {avg_years:.1f} years\")\n",
    "    \n",
    "    # Characterize the cluster\n",
    "    if avg_income > df['Annual_Income'].mean() and avg_spending > df['Spending_Score'].mean():\n",
    "        profile = \"HIGH-VALUE CUSTOMERS\"\n",
    "    elif avg_income < df['Annual_Income'].mean() and avg_spending < df['Spending_Score'].mean():\n",
    "        profile = \"BUDGET-CONSCIOUS CUSTOMERS\"\n",
    "    elif avg_spending > df['Spending_Score'].mean():\n",
    "        profile = \"HIGH-SPENDING CUSTOMERS\"\n",
    "    elif avg_frequency > df['Purchase_Frequency'].mean():\n",
    "        profile = \"FREQUENT BUYERS\"\n",
    "    else:\n",
    "        profile = \"MODERATE CUSTOMERS\"\n",
    "    \n",
    "    print(f\"Profile: {profile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1093fa",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bdccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Scatter Plots for Cluster Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Customer Segmentation - 2D Cluster Visualizations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Color palette for clusters\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, optimal_k))\n",
    "\n",
    "# Plot 1: Annual Income vs Spending Score\n",
    "scatter1 = axes[0, 0].scatter(df['Annual_Income'], df['Spending_Score'], \n",
    "                             c=df['Cluster'], cmap='Set1', alpha=0.7, s=50)\n",
    "axes[0, 0].set_xlabel('Annual Income ($)')\n",
    "axes[0, 0].set_ylabel('Spending Score (1-100)')\n",
    "axes[0, 0].set_title('Income vs Spending Score')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Age vs Spending Score\n",
    "scatter2 = axes[0, 1].scatter(df['Age'], df['Spending_Score'], \n",
    "                             c=df['Cluster'], cmap='Set1', alpha=0.7, s=50)\n",
    "axes[0, 1].set_xlabel('Age (years)')\n",
    "axes[0, 1].set_ylabel('Spending Score (1-100)')\n",
    "axes[0, 1].set_title('Age vs Spending Score')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Purchase Frequency vs Average Order Value\n",
    "scatter3 = axes[1, 0].scatter(df['Purchase_Frequency'], df['Average_Order_Value'], \n",
    "                             c=df['Cluster'], cmap='Set1', alpha=0.7, s=50)\n",
    "axes[1, 0].set_xlabel('Purchase Frequency')\n",
    "axes[1, 0].set_ylabel('Average Order Value ($)')\n",
    "axes[1, 0].set_title('Purchase Frequency vs Order Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Age vs Annual Income\n",
    "scatter4 = axes[1, 1].scatter(df['Age'], df['Annual_Income'], \n",
    "                             c=df['Cluster'], cmap='Set1', alpha=0.7, s=50)\n",
    "axes[1, 1].set_xlabel('Age (years)')\n",
    "axes[1, 1].set_ylabel('Annual Income ($)')\n",
    "axes[1, 1].set_title('Age vs Annual Income')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(scatter1, ax=axes, orientation='horizontal', \n",
    "            label='Cluster', shrink=0.8, pad=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515322e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Visualization\n",
    "print(\"Applying PCA for dimensionality reduction...\")\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2D for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create PCA DataFrame\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Cluster'], \n",
    "                     cmap='Set1', alpha=0.7, s=60)\n",
    "\n",
    "# Plot cluster centroids in PCA space\n",
    "centroids_pca = pca.transform(kmeans_optimal.cluster_centers_)\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "           c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "\n",
    "plt.xlabel(f'First Principal Component (Explained Variance: {pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'Second Principal Component (Explained Variance: {pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.title('Customer Clusters - PCA Visualization')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total explained variance by 2 components: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "print(f\"PCA Components contribution:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "\n",
    "# Show feature contributions to principal components\n",
    "feature_importance = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=features_for_clustering\n",
    ")\n",
    "print(f\"\\nFeature contributions to Principal Components:\")\n",
    "print(feature_importance.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Visualization\n",
    "print(\"Applying t-SNE for non-linear dimensionality reduction...\")\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Create t-SNE DataFrame\n",
    "tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2'])\n",
    "tsne_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Cluster'], \n",
    "                     cmap='Set1', alpha=0.7, s=60)\n",
    "\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('Customer Clusters - t-SNE Visualization')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a611f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar Chart for Cluster Comparison\n",
    "def create_radar_chart(df, features, cluster_col='Cluster'):\n",
    "    \"\"\"\n",
    "    Create radar chart to compare cluster characteristics\n",
    "    \"\"\"\n",
    "    # Normalize features to 0-1 scale for radar chart\n",
    "    df_norm = df.copy()\n",
    "    for feature in features:\n",
    "        df_norm[feature] = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())\n",
    "    \n",
    "    # Calculate mean values for each cluster\n",
    "    cluster_means = df_norm.groupby(cluster_col)[features].mean()\n",
    "    \n",
    "    # Set up the figure\n",
    "    num_clusters = len(cluster_means)\n",
    "    angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, num_clusters))\n",
    "    \n",
    "    for idx, (cluster_id, values) in enumerate(cluster_means.iterrows()):\n",
    "        values_list = values.tolist()\n",
    "        values_list += values_list[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values_list, 'o-', linewidth=2, \n",
    "                label=f'Cluster {cluster_id}', color=colors[idx])\n",
    "        ax.fill(angles, values_list, alpha=0.25, color=colors[idx])\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(features)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Cluster Characteristics Comparison\\n(Normalized Values)', \n",
    "                size=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create radar chart\n",
    "print(\"Creating radar chart for cluster comparison...\")\n",
    "create_radar_chart(df, features_for_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795d80c",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and preprocessing components\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "if IN_COLAB:\n",
    "    # In Colab, save to current directory\n",
    "    models_dir = '/content/models'\n",
    "else:\n",
    "    # Local environment\n",
    "    models_dir = 'models'\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the K-means model\n",
    "joblib.dump(kmeans_optimal, f'{models_dir}/kmeans_customer_segmentation.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, f'{models_dir}/scaler.pkl')\n",
    "\n",
    "# Save the PCA model\n",
    "joblib.dump(pca, f'{models_dir}/pca_model.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "print(f\"Models saved to: {models_dir}\")\n",
    "\n",
    "# List saved files\n",
    "if os.path.exists(models_dir):\n",
    "    saved_files = os.listdir(models_dir)\n",
    "    print(f\"Saved files: {saved_files}\")\n",
    "\n",
    "# Create a function to predict cluster for new customers\n",
    "def predict_customer_segment(customer_data, models_dir=None):\n",
    "    \"\"\"\n",
    "    Predict cluster for new customer data\n",
    "    \n",
    "    Parameters:\n",
    "    customer_data: dict or DataFrame with customer features\n",
    "    models_dir: path to models directory (auto-detected if None)\n",
    "    \n",
    "    Returns:\n",
    "    cluster_id: predicted cluster\n",
    "    \"\"\"\n",
    "    # Auto-detect models directory\n",
    "    if models_dir is None:\n",
    "        if IN_COLAB:\n",
    "            models_dir = '/content/models'\n",
    "        else:\n",
    "            models_dir = 'models'\n",
    "    \n",
    "    model_path = f'{models_dir}/kmeans_customer_segmentation.pkl'\n",
    "    scaler_path = f'{models_dir}/scaler.pkl'\n",
    "    \n",
    "    # Load models\n",
    "    kmeans_model = joblib.load(model_path)\n",
    "    scaler_model = joblib.load(scaler_path)\n",
    "    \n",
    "    # Convert to DataFrame if it's a dictionary\n",
    "    if isinstance(customer_data, dict):\n",
    "        customer_df = pd.DataFrame([customer_data])\n",
    "    else:\n",
    "        customer_df = customer_data.copy()\n",
    "    \n",
    "    # Select features and scale\n",
    "    features = ['Age', 'Annual_Income', 'Spending_Score', \n",
    "               'Purchase_Frequency', 'Average_Order_Value', 'Years_Customer']\n",
    "    \n",
    "    customer_features = customer_df[features]\n",
    "    customer_scaled = scaler_model.transform(customer_features)\n",
    "    \n",
    "    # Predict cluster\n",
    "    predicted_cluster = kmeans_model.predict(customer_scaled)\n",
    "    \n",
    "    return predicted_cluster[0] if len(predicted_cluster) == 1 else predicted_cluster\n",
    "\n",
    "# Test the prediction function with a sample customer\n",
    "sample_customer = {\n",
    "    'Age': 35,\n",
    "    'Annual_Income': 50000,\n",
    "    'Spending_Score': 75,\n",
    "    'Purchase_Frequency': 20,\n",
    "    'Average_Order_Value': 120,\n",
    "    'Years_Customer': 3\n",
    "}\n",
    "\n",
    "predicted_segment = predict_customer_segment(sample_customer)\n",
    "print(f\"\\nSample customer prediction:\")\n",
    "print(f\"Customer data: {sample_customer}\")\n",
    "print(f\"Predicted cluster: {predicted_segment}\")\n",
    "\n",
    "# Create interpretation function\n",
    "def interpret_cluster(cluster_id):\n",
    "    \"\"\"\n",
    "    Provide business interpretation of cluster\n",
    "    \"\"\"\n",
    "    cluster_profiles = {\n",
    "        0: \"Budget-conscious customers with moderate spending patterns\",\n",
    "        1: \"High-value customers with strong purchasing power\",\n",
    "        2: \"Frequent buyers with consistent engagement\",\n",
    "        3: \"Premium customers with high order values\",\n",
    "        4: \"New or occasional customers with growth potential\"\n",
    "    }\n",
    "    \n",
    "    # Use generic interpretation if cluster_id not in predefined profiles\n",
    "    if cluster_id in cluster_profiles:\n",
    "        return cluster_profiles[cluster_id]\n",
    "    else:\n",
    "        return f\"Customer segment {cluster_id} - requires further analysis\"\n",
    "\n",
    "print(f\"Interpretation: {interpret_cluster(predicted_segment)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33d6f6",
   "metadata": {},
   "source": [
    "## üöÄ Model Deployment Options\n",
    "\n",
    "Now that we have trained our model and saved the files, let's create different deployment options:\n",
    "\n",
    "### Deployment Files Created:\n",
    "1. `kmeans_customer_segmentation.pkl` - Trained K-Means model\n",
    "2. `scaler.pkl` - Feature scaler for preprocessing\n",
    "3. `pca_model.pkl` - PCA model for visualization\n",
    "\n",
    "### Deployment Options Available:\n",
    "1. **Streamlit Web App** - Interactive web interface\n",
    "2. **Flask API** - REST API for integration\n",
    "3. **Gradio Interface** - Quick ML demo interface\n",
    "4. **Standalone Python Script** - Command-line tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Streamlit Web App for Model Deployment\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Customer Segmentation App\",\n",
    "    page_icon=\"üéØ\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Load models\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    try:\n",
    "        kmeans_model = joblib.load('models/kmeans_customer_segmentation.pkl')\n",
    "        scaler = joblib.load('models/scaler.pkl')\n",
    "        pca_model = joblib.load('models/pca_model.pkl')\n",
    "        return kmeans_model, scaler, pca_model\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading models: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Prediction function\n",
    "def predict_customer_segment(customer_data, kmeans_model, scaler):\n",
    "    features = ['Age', 'Annual_Income', 'Spending_Score', \n",
    "                'Purchase_Frequency', 'Average_Order_Value', 'Years_Customer']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([customer_data])\n",
    "    \n",
    "    # Select and scale features\n",
    "    X = df[features]\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Predict cluster\n",
    "    cluster = kmeans_model.predict(X_scaled)[0]\n",
    "    \n",
    "    # Get cluster probabilities (distances to centroids)\n",
    "    distances = kmeans_model.transform(X_scaled)[0]\n",
    "    probabilities = 1 / (1 + distances)\n",
    "    probabilities = probabilities / probabilities.sum()\n",
    "    \n",
    "    return cluster, probabilities\n",
    "\n",
    "# Cluster interpretation\n",
    "def interpret_cluster(cluster_id):\n",
    "    cluster_profiles = {\n",
    "        0: \"Budget-conscious customers with moderate spending patterns\",\n",
    "        1: \"High-value customers with strong purchasing power\", \n",
    "        2: \"Frequent buyers with consistent engagement\",\n",
    "        3: \"Premium customers with high order values\",\n",
    "        4: \"New or occasional customers with growth potential\"\n",
    "    }\n",
    "    return cluster_profiles.get(cluster_id, f\"Customer segment {cluster_id}\")\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"üéØ Customer Segmentation Prediction\")\n",
    "    st.markdown(\"Predict customer segments using K-Means clustering\")\n",
    "    \n",
    "    # Load models\n",
    "    kmeans_model, scaler, pca_model = load_models()\n",
    "    \n",
    "    if kmeans_model is None:\n",
    "        st.error(\"Models not found. Please ensure model files are in the 'models' directory.\")\n",
    "        return\n",
    "    \n",
    "    # Sidebar for input\n",
    "    st.sidebar.header(\"Customer Information\")\n",
    "    \n",
    "    # Input fields\n",
    "    age = st.sidebar.slider(\"Age\", 18, 80, 35)\n",
    "    annual_income = st.sidebar.slider(\"Annual Income ($)\", 20000, 150000, 60000)\n",
    "    spending_score = st.sidebar.slider(\"Spending Score (1-100)\", 1, 100, 50)\n",
    "    purchase_frequency = st.sidebar.slider(\"Purchase Frequency\", 1, 50, 15)\n",
    "    avg_order_value = st.sidebar.slider(\"Average Order Value ($)\", 20, 500, 150)\n",
    "    years_customer = st.sidebar.slider(\"Years as Customer\", 0.1, 10.0, 2.0)\n",
    "    \n",
    "    # Create customer data\n",
    "    customer_data = {\n",
    "        'Age': age,\n",
    "        'Annual_Income': annual_income,\n",
    "        'Spending_Score': spending_score,\n",
    "        'Purchase_Frequency': purchase_frequency,\n",
    "        'Average_Order_Value': avg_order_value,\n",
    "        'Years_Customer': years_customer\n",
    "    }\n",
    "    \n",
    "    # Predict button\n",
    "    if st.sidebar.button(\"Predict Segment\", type=\"primary\"):\n",
    "        cluster, probabilities = predict_customer_segment(customer_data, kmeans_model, scaler)\n",
    "        \n",
    "        # Display results\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Prediction Results\")\n",
    "            st.success(f\"**Predicted Cluster: {cluster}**\")\n",
    "            st.info(f\"**Profile:** {interpret_cluster(cluster)}\")\n",
    "            \n",
    "            # Customer summary\n",
    "            st.subheader(\"Customer Summary\")\n",
    "            summary_df = pd.DataFrame([customer_data]).T\n",
    "            summary_df.columns = ['Value']\n",
    "            st.dataframe(summary_df, use_container_width=True)\n",
    "            \n",
    "        with col2:\n",
    "            st.subheader(\"Cluster Probabilities\")\n",
    "            prob_df = pd.DataFrame({\n",
    "                'Cluster': [f'Cluster {i}' for i in range(len(probabilities))],\n",
    "                'Probability': probabilities\n",
    "            })\n",
    "            \n",
    "            fig = px.bar(prob_df, x='Cluster', y='Probability', \n",
    "                        title='Cluster Assignment Confidence')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Batch prediction\n",
    "    st.subheader(\"üìä Batch Prediction\")\n",
    "    uploaded_file = st.file_uploader(\"Upload CSV file for batch prediction\", type=['csv'])\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            batch_df = pd.read_csv(uploaded_file)\n",
    "            st.write(\"Uploaded data:\", batch_df.head())\n",
    "            \n",
    "            if st.button(\"Predict All\"):\n",
    "                # Predict for all customers\n",
    "                predictions = []\n",
    "                for _, row in batch_df.iterrows():\n",
    "                    cluster, _ = predict_customer_segment(row.to_dict(), kmeans_model, scaler)\n",
    "                    predictions.append(cluster)\n",
    "                \n",
    "                batch_df['Predicted_Cluster'] = predictions\n",
    "                batch_df['Cluster_Profile'] = batch_df['Predicted_Cluster'].apply(interpret_cluster)\n",
    "                \n",
    "                st.success(\"Predictions completed!\")\n",
    "                st.dataframe(batch_df)\n",
    "                \n",
    "                # Download results\n",
    "                csv = batch_df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"Download Results\",\n",
    "                    data=csv,\n",
    "                    file_name=\"customer_predictions.csv\",\n",
    "                    mime=\"text/csv\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error processing file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save Streamlit app\n",
    "app_filename = 'customer_segmentation_app.py'\n",
    "if IN_COLAB:\n",
    "    app_path = f'/content/{app_filename}'\n",
    "else:\n",
    "    app_path = app_filename\n",
    "\n",
    "with open(app_path, 'w') as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(f\"‚úÖ Streamlit app created: {app_path}\")\n",
    "print(\"\\nüìù To run the Streamlit app:\")\n",
    "print(f\"   streamlit run {app_filename}\")\n",
    "\n",
    "# Download in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(app_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca844af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Flask API for Model Deployment\n",
    "flask_api_code = '''\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    kmeans_model = joblib.load('models/kmeans_customer_segmentation.pkl')\n",
    "    scaler = joblib.load('models/scaler.pkl')\n",
    "    pca_model = joblib.load('models/pca_model.pkl')\n",
    "    print(\"‚úÖ Models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading models: {e}\")\n",
    "    kmeans_model = scaler = pca_model = None\n",
    "\n",
    "def predict_customer_segment(customer_data):\n",
    "    \"\"\"Predict customer segment\"\"\"\n",
    "    features = ['Age', 'Annual_Income', 'Spending_Score', \n",
    "                'Purchase_Frequency', 'Average_Order_Value', 'Years_Customer']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([customer_data])\n",
    "    \n",
    "    # Select and scale features\n",
    "    X = df[features]\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Predict cluster\n",
    "    cluster = kmeans_model.predict(X_scaled)[0]\n",
    "    \n",
    "    # Get prediction confidence (distance to centroids)\n",
    "    distances = kmeans_model.transform(X_scaled)[0]\n",
    "    confidence = 1 / (1 + distances.min())\n",
    "    \n",
    "    return int(cluster), float(confidence)\n",
    "\n",
    "def interpret_cluster(cluster_id):\n",
    "    \"\"\"Interpret cluster meaning\"\"\"\n",
    "    cluster_profiles = {\n",
    "        0: \"Budget-conscious customers with moderate spending patterns\",\n",
    "        1: \"High-value customers with strong purchasing power\",\n",
    "        2: \"Frequent buyers with consistent engagement\", \n",
    "        3: \"Premium customers with high order values\",\n",
    "        4: \"New or occasional customers with growth potential\"\n",
    "    }\n",
    "    return cluster_profiles.get(cluster_id, f\"Customer segment {cluster_id}\")\n",
    "\n",
    "# HTML template for web interface\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Customer Segmentation API</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "        .container { max-width: 800px; margin: 0 auto; }\n",
    "        .form-group { margin: 15px 0; }\n",
    "        label { display: block; margin-bottom: 5px; font-weight: bold; }\n",
    "        input { width: 100%; padding: 8px; border: 1px solid #ddd; border-radius: 4px; }\n",
    "        button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; }\n",
    "        button:hover { background: #0056b3; }\n",
    "        .result { margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 4px; }\n",
    "        .api-docs { margin-top: 30px; padding: 20px; background: #e9ecef; border-radius: 4px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>üéØ Customer Segmentation API</h1>\n",
    "        \n",
    "        <h2>Test the API</h2>\n",
    "        <form id=\"predictionForm\">\n",
    "            <div class=\"form-group\">\n",
    "                <label>Age:</label>\n",
    "                <input type=\"number\" id=\"age\" min=\"18\" max=\"80\" value=\"35\" required>\n",
    "            </div>\n",
    "            <div class=\"form-group\">\n",
    "                <label>Annual Income ($):</label>\n",
    "                <input type=\"number\" id=\"annual_income\" min=\"20000\" max=\"150000\" value=\"60000\" required>\n",
    "            </div>\n",
    "            <div class=\"form-group\">\n",
    "                <label>Spending Score (1-100):</label>\n",
    "                <input type=\"number\" id=\"spending_score\" min=\"1\" max=\"100\" value=\"50\" required>\n",
    "            </div>\n",
    "            <div class=\"form-group\">\n",
    "                <label>Purchase Frequency:</label>\n",
    "                <input type=\"number\" id=\"purchase_frequency\" min=\"1\" max=\"50\" value=\"15\" required>\n",
    "            </div>\n",
    "            <div class=\"form-group\">\n",
    "                <label>Average Order Value ($):</label>\n",
    "                <input type=\"number\" id=\"avg_order_value\" min=\"20\" max=\"500\" value=\"150\" required>\n",
    "            </div>\n",
    "            <div class=\"form-group\">\n",
    "                <label>Years as Customer:</label>\n",
    "                <input type=\"number\" id=\"years_customer\" min=\"0.1\" max=\"10\" step=\"0.1\" value=\"2.0\" required>\n",
    "            </div>\n",
    "            <button type=\"submit\">Predict Segment</button>\n",
    "        </form>\n",
    "        \n",
    "        <div id=\"result\" class=\"result\" style=\"display:none;\"></div>\n",
    "        \n",
    "        <div class=\"api-docs\">\n",
    "            <h3>API Documentation</h3>\n",
    "            <p><strong>Endpoint:</strong> POST /predict</p>\n",
    "            <p><strong>Content-Type:</strong> application/json</p>\n",
    "            <p><strong>Example Request:</strong></p>\n",
    "            <pre>{\n",
    "  \"Age\": 35,\n",
    "  \"Annual_Income\": 60000,\n",
    "  \"Spending_Score\": 75,\n",
    "  \"Purchase_Frequency\": 20,\n",
    "  \"Average_Order_Value\": 120,\n",
    "  \"Years_Customer\": 3.0\n",
    "}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        document.getElementById('predictionForm').addEventListener('submit', async (e) => {\n",
    "            e.preventDefault();\n",
    "            \n",
    "            const data = {\n",
    "                Age: parseInt(document.getElementById('age').value),\n",
    "                Annual_Income: parseInt(document.getElementById('annual_income').value),\n",
    "                Spending_Score: parseInt(document.getElementById('spending_score').value),\n",
    "                Purchase_Frequency: parseInt(document.getElementById('purchase_frequency').value),\n",
    "                Average_Order_Value: parseInt(document.getElementById('avg_order_value').value),\n",
    "                Years_Customer: parseFloat(document.getElementById('years_customer').value)\n",
    "            };\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch('/predict', {\n",
    "                    method: 'POST',\n",
    "                    headers: { 'Content-Type': 'application/json' },\n",
    "                    body: JSON.stringify(data)\n",
    "                });\n",
    "                \n",
    "                const result = await response.json();\n",
    "                \n",
    "                document.getElementById('result').innerHTML = `\n",
    "                    <h3>Prediction Result</h3>\n",
    "                    <p><strong>Cluster:</strong> ${result.cluster}</p>\n",
    "                    <p><strong>Profile:</strong> ${result.interpretation}</p>\n",
    "                    <p><strong>Confidence:</strong> ${(result.confidence * 100).toFixed(1)}%</p>\n",
    "                `;\n",
    "                document.getElementById('result').style.display = 'block';\n",
    "                \n",
    "            } catch (error) {\n",
    "                document.getElementById('result').innerHTML = `<p style=\"color:red;\">Error: ${error.message}</p>`;\n",
    "                document.getElementById('result').style.display = 'block';\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"Home page with web interface\"\"\"\n",
    "    return render_template_string(html_template)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"API endpoint for predictions\"\"\"\n",
    "    try:\n",
    "        if kmeans_model is None:\n",
    "            return jsonify({'error': 'Models not loaded'}), 500\n",
    "        \n",
    "        data = request.json\n",
    "        \n",
    "        # Validate required fields\n",
    "        required_fields = ['Age', 'Annual_Income', 'Spending_Score', \n",
    "                          'Purchase_Frequency', 'Average_Order_Value', 'Years_Customer']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                return jsonify({'error': f'Missing field: {field}'}), 400\n",
    "        \n",
    "        # Make prediction\n",
    "        cluster, confidence = predict_customer_segment(data)\n",
    "        interpretation = interpret_cluster(cluster)\n",
    "        \n",
    "        return jsonify({\n",
    "            'cluster': cluster,\n",
    "            'interpretation': interpretation,\n",
    "            'confidence': confidence,\n",
    "            'input_data': data\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/health')\n",
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'models_loaded': kmeans_model is not None\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"üöÄ Starting Customer Segmentation API...\")\n",
    "    print(\"üìç Access the web interface at: http://localhost:5000\")\n",
    "    print(\"üìç API endpoint: http://localhost:5000/predict\")\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "'''\n",
    "\n",
    "# Save Flask API\n",
    "api_filename = 'customer_segmentation_api.py'\n",
    "if IN_COLAB:\n",
    "    api_path = f'/content/{api_filename}'\n",
    "else:\n",
    "    api_path = api_filename\n",
    "\n",
    "with open(api_path, 'w') as f:\n",
    "    f.write(flask_api_code)\n",
    "\n",
    "print(f\"‚úÖ Flask API created: {api_path}\")\n",
    "print(\"\\nüìù To run the Flask API:\")\n",
    "print(f\"   python {api_filename}\")\n",
    "print(\"   Then visit: http://localhost:5000\")\n",
    "\n",
    "# Download in Colab\n",
    "if IN_COLAB:\n",
    "    files.download(api_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9560e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Gradio Interface (Quick ML Demo)\n",
    "try:\n",
    "    import gradio as gr\n",
    "    GRADIO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing Gradio...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gradio\"])\n",
    "    import gradio as gr\n",
    "    GRADIO_AVAILABLE = True\n",
    "\n",
    "if GRADIO_AVAILABLE:\n",
    "    def gradio_predict(age, annual_income, spending_score, purchase_frequency, avg_order_value, years_customer):\n",
    "        \"\"\"Gradio prediction function\"\"\"\n",
    "        customer_data = {\n",
    "            'Age': age,\n",
    "            'Annual_Income': annual_income,\n",
    "            'Spending_Score': spending_score,\n",
    "            'Purchase_Frequency': purchase_frequency,\n",
    "            'Average_Order_Value': avg_order_value,\n",
    "            'Years_Customer': years_customer\n",
    "        }\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_cluster = predict_customer_segment(customer_data)\n",
    "        interpretation = interpret_cluster(predicted_cluster)\n",
    "        \n",
    "        return f\"Cluster: {predicted_cluster}\", interpretation\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=gradio_predict,\n",
    "        inputs=[\n",
    "            gr.Slider(18, 80, value=35, label=\"Age\"),\n",
    "            gr.Slider(20000, 150000, value=60000, label=\"Annual Income ($)\"),\n",
    "            gr.Slider(1, 100, value=50, label=\"Spending Score (1-100)\"),\n",
    "            gr.Slider(1, 50, value=15, label=\"Purchase Frequency\"),\n",
    "            gr.Slider(20, 500, value=150, label=\"Average Order Value ($)\"),\n",
    "            gr.Slider(0.1, 10.0, value=2.0, label=\"Years as Customer\")\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Predicted Cluster\"),\n",
    "            gr.Textbox(label=\"Customer Profile\")\n",
    "        ],\n",
    "        title=\"üéØ Customer Segmentation Predictor\",\n",
    "        description=\"Enter customer details to predict their segment using K-Means clustering\",\n",
    "        examples=[\n",
    "            [25, 30000, 20, 5, 80, 0.5],  # Budget customer\n",
    "            [45, 80000, 85, 25, 200, 5.0],  # High-value customer\n",
    "            [35, 50000, 60, 30, 120, 3.0],  # Frequent buyer\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Gradio interface created!\")\n",
    "    print(\"üìù To launch Gradio interface:\")\n",
    "    print(\"   interface.launch()\")\n",
    "    \n",
    "    # Launch interface (uncomment to run)\n",
    "    # interface.launch(share=True)  # share=True creates public link\n",
    "else:\n",
    "    print(\"‚ùå Gradio not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e564af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Standalone Deployment Script\n",
    "standalone_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Customer Segmentation Model - Standalone Deployment Script\n",
    "Usage: python customer_segmentation_standalone.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class CustomerSegmentationPredictor:\n",
    "    def __init__(self, models_dir='models'):\n",
    "        \"\"\"Initialize the predictor with model files\"\"\"\n",
    "        self.models_dir = models_dir\n",
    "        self.kmeans_model = None\n",
    "        self.scaler = None\n",
    "        self.pca_model = None\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load the trained models\"\"\"\n",
    "        try:\n",
    "            self.kmeans_model = joblib.load(f'{self.models_dir}/kmeans_customer_segmentation.pkl')\n",
    "            self.scaler = joblib.load(f'{self.models_dir}/scaler.pkl')\n",
    "            self.pca_model = joblib.load(f'{self.models_dir}/pca_model.pkl')\n",
    "            print(\"‚úÖ Models loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading models: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def predict_single(self, customer_data):\n",
    "        \"\"\"Predict segment for a single customer\"\"\"\n",
    "        features = ['Age', 'Annual_Income', 'Spending_Score', \n",
    "                   'Purchase_Frequency', 'Average_Order_Value', 'Years_Customer']\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame([customer_data])\n",
    "        \n",
    "        # Select and scale features\n",
    "        X = df[features]\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Predict cluster\n",
    "        cluster = self.kmeans_model.predict(X_scaled)[0]\n",
    "        \n",
    "        # Get prediction confidence\n",
    "        distances = self.kmeans_model.transform(X_scaled)[0]\n",
    "        confidence = 1 / (1 + distances.min())\n",
    "        \n",
    "        return {\n",
    "            'cluster': int(cluster),\n",
    "            'confidence': float(confidence),\n",
    "            'interpretation': self.interpret_cluster(cluster)\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, csv_file):\n",
    "        \"\"\"Predict segments for multiple customers from CSV\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            results = []\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                result = self.predict_single(row.to_dict())\n",
    "                results.append(result)\n",
    "            \n",
    "            # Add results to dataframe\n",
    "            df['Predicted_Cluster'] = [r['cluster'] for r in results]\n",
    "            df['Confidence'] = [r['confidence'] for r in results]\n",
    "            df['Interpretation'] = [r['interpretation'] for r in results]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing batch file: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def interpret_cluster(self, cluster_id):\n",
    "        \"\"\"Interpret cluster meaning\"\"\"\n",
    "        cluster_profiles = {\n",
    "            0: \"Budget-conscious customers with moderate spending patterns\",\n",
    "            1: \"High-value customers with strong purchasing power\",\n",
    "            2: \"Frequent buyers with consistent engagement\",\n",
    "            3: \"Premium customers with high order values\", \n",
    "            4: \"New or occasional customers with growth potential\"\n",
    "        }\n",
    "        return cluster_profiles.get(cluster_id, f\"Customer segment {cluster_id}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Customer Segmentation Predictor')\n",
    "    parser.add_argument('--mode', choices=['single', 'batch', 'interactive'], \n",
    "                       default='interactive', help='Prediction mode')\n",
    "    parser.add_argument('--input', help='Input file for batch mode')\n",
    "    parser.add_argument('--output', help='Output file for batch mode')\n",
    "    parser.add_argument('--models-dir', default='models', help='Models directory')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize predictor\n",
    "    predictor = CustomerSegmentationPredictor(args.models_dir)\n",
    "    \n",
    "    if args.mode == 'single':\n",
    "        # Single prediction with command line inputs\n",
    "        print(\"Enter customer details:\")\n",
    "        age = int(input(\"Age: \"))\n",
    "        annual_income = int(input(\"Annual Income ($): \"))\n",
    "        spending_score = int(input(\"Spending Score (1-100): \"))\n",
    "        purchase_frequency = int(input(\"Purchase Frequency: \"))\n",
    "        avg_order_value = float(input(\"Average Order Value ($): \"))\n",
    "        years_customer = float(input(\"Years as Customer: \"))\n",
    "        \n",
    "        customer_data = {\n",
    "            'Age': age,\n",
    "            'Annual_Income': annual_income,\n",
    "            'Spending_Score': spending_score,\n",
    "            'Purchase_Frequency': purchase_frequency,\n",
    "            'Average_Order_Value': avg_order_value,\n",
    "            'Years_Customer': years_customer\n",
    "        }\n",
    "        \n",
    "        result = predictor.predict_single(customer_data)\n",
    "        print(f\"\\\\nüéØ Prediction Results:\")\n",
    "        print(f\"Cluster: {result['cluster']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "        print(f\"Profile: {result['interpretation']}\")\n",
    "        \n",
    "    elif args.mode == 'batch':\n",
    "        if not args.input:\n",
    "            print(\"‚ùå Input file required for batch mode\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        print(f\"üìä Processing batch file: {args.input}\")\n",
    "        results_df = predictor.predict_batch(args.input)\n",
    "        \n",
    "        if results_df is not None:\n",
    "            output_file = args.output or args.input.replace('.csv', '_predictions.csv')\n",
    "            results_df.to_csv(output_file, index=False)\n",
    "            print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "    \n",
    "    elif args.mode == 'interactive':\n",
    "        print(\"üéØ Customer Segmentation Predictor - Interactive Mode\")\n",
    "        print(\"Enter 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(\"\\\\n\" + \"=\"*50)\n",
    "                age = input(\"Age (or 'quit'): \")\n",
    "                if age.lower() == 'quit':\n",
    "                    break\n",
    "                \n",
    "                annual_income = input(\"Annual Income ($): \")\n",
    "                spending_score = input(\"Spending Score (1-100): \")\n",
    "                purchase_frequency = input(\"Purchase Frequency: \")\n",
    "                avg_order_value = input(\"Average Order Value ($): \")\n",
    "                years_customer = input(\"Years as Customer: \")\n",
    "                \n",
    "                customer_data = {\n",
    "                    'Age': int(age),\n",
    "                    'Annual_Income': int(annual_income),\n",
    "                    'Spending_Score': int(spending_score),\n",
    "                    'Purchase_Frequency': int(purchase_frequency),\n",
    "                    'Average_Order_Value': float(avg_order_value),\n",
    "                    'Years_Customer': float(years_customer)\n",
    "                }\n",
    "                \n",
    "                result = predictor.predict_single(customer_data)\n",
    "                print(f\"\\\\nüéØ Results:\")\n",
    "                print(f\"Cluster: {result['cluster']}\")\n",
    "                print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "                print(f\"Profile: {result['interpretation']}\")\n",
    "                \n",
    "            except (ValueError, KeyboardInterrupt):\n",
    "                print(\"\\\\nüëã Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save standalone script\n",
    "standalone_filename = 'customer_segmentation_standalone.py'\n",
    "if IN_COLAB:\n",
    "    standalone_path = f'/content/{standalone_filename}'\n",
    "else:\n",
    "    standalone_path = standalone_filename\n",
    "\n",
    "with open(standalone_path, 'w') as f:\n",
    "    f.write(standalone_script)\n",
    "\n",
    "print(f\"‚úÖ Standalone script created: {standalone_path}\")\n",
    "\n",
    "# Create requirements.txt\n",
    "requirements_content = '''pandas>=1.3.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "joblib>=1.0.0\n",
    "streamlit>=1.0.0\n",
    "flask>=2.0.0\n",
    "gradio>=3.0.0\n",
    "plotly>=5.0.0\n",
    "'''\n",
    "\n",
    "requirements_filename = 'requirements.txt'\n",
    "if IN_COLAB:\n",
    "    req_path = f'/content/{requirements_filename}'\n",
    "else:\n",
    "    req_path = requirements_filename\n",
    "\n",
    "with open(req_path, 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(f\"‚úÖ Requirements file created: {req_path}\")\n",
    "\n",
    "# Create README for deployment\n",
    "readme_content = '''# Customer Segmentation Model Deployment\n",
    "\n",
    "## Files Generated:\n",
    "1. `customer_segmentation_app.py` - Streamlit web app\n",
    "2. `customer_segmentation_api.py` - Flask REST API  \n",
    "3. `customer_segmentation_standalone.py` - Command-line tool\n",
    "4. `requirements.txt` - Python dependencies\n",
    "5. `models/` directory with trained models\n",
    "\n",
    "## Deployment Options:\n",
    "\n",
    "### 1. Streamlit Web App\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "streamlit run customer_segmentation_app.py\n",
    "```\n",
    "\n",
    "### 2. Flask API\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python customer_segmentation_api.py\n",
    "```\n",
    "\n",
    "### 3. Standalone Script\n",
    "```bash\n",
    "# Interactive mode\n",
    "python customer_segmentation_standalone.py --mode interactive\n",
    "\n",
    "# Single prediction\n",
    "python customer_segmentation_standalone.py --mode single\n",
    "\n",
    "# Batch processing\n",
    "python customer_segmentation_standalone.py --mode batch --input customers.csv\n",
    "```\n",
    "\n",
    "### 4. Docker Deployment\n",
    "Create a Dockerfile:\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY . .\n",
    "EXPOSE 5000\n",
    "CMD [\"python\", \"customer_segmentation_api.py\"]\n",
    "```\n",
    "\n",
    "## Model Files:\n",
    "- `kmeans_customer_segmentation.pkl` - Trained K-Means model\n",
    "- `scaler.pkl` - Feature scaler\n",
    "- `pca_model.pkl` - PCA model for visualization\n",
    "'''\n",
    "\n",
    "readme_filename = 'README_deployment.md'\n",
    "if IN_COLAB:\n",
    "    readme_path = f'/content/{readme_filename}'\n",
    "else:\n",
    "    readme_path = readme_filename\n",
    "\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"‚úÖ Deployment README created: {readme_path}\")\n",
    "\n",
    "# Download all files in Colab\n",
    "if IN_COLAB:\n",
    "    files.download(standalone_path)\n",
    "    files.download(req_path)\n",
    "    files.download(readme_path)\n",
    "\n",
    "print(\"\\nüöÄ Deployment package created successfully!\")\n",
    "print(\"üìÅ Files generated:\")\n",
    "print(\"   - Streamlit app\")\n",
    "print(\"   - Flask API\")\n",
    "print(\"   - Standalone script\")\n",
    "print(\"   - Requirements.txt\")\n",
    "print(\"   - Deployment README\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed22f9",
   "metadata": {},
   "source": [
    "## üéâ Deployment Complete!\n",
    "\n",
    "Your customer segmentation model is now fully deployed with multiple options:\n",
    "\n",
    "### üìÅ Generated Files:\n",
    "\n",
    "1. **`customer_segmentation_app.py`** - Interactive Streamlit web application\n",
    "2. **`customer_segmentation_api.py`** - REST API with Flask backend  \n",
    "3. **`customer_segmentation_standalone.py`** - Command-line prediction tool\n",
    "4. **`requirements.txt`** - All Python dependencies\n",
    "5. **`README_deployment.md`** - Deployment instructions\n",
    "\n",
    "### üöÄ Quick Start:\n",
    "\n",
    "#### Option 1: Streamlit Web App (Recommended for demos)\n",
    "```bash\n",
    "streamlit run customer_segmentation_app.py\n",
    "```\n",
    "- Interactive web interface\n",
    "- Real-time predictions\n",
    "- Batch file upload\n",
    "- Visualization charts\n",
    "\n",
    "#### Option 2: Flask API (Best for integration)\n",
    "```bash\n",
    "python customer_segmentation_api.py\n",
    "```\n",
    "- RESTful API endpoints\n",
    "- JSON request/response\n",
    "- Web interface included\n",
    "- Easy integration with other systems\n",
    "\n",
    "#### Option 3: Command Line Tool\n",
    "```bash\n",
    "python customer_segmentation_standalone.py --mode interactive\n",
    "```\n",
    "- No dependencies on web frameworks\n",
    "- Batch processing capabilities\n",
    "- Perfect for automated workflows\n",
    "\n",
    "### üîß Installation:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### üìä Model Performance:\n",
    "- **Clusters**: Optimal number determined by silhouette analysis\n",
    "- **Features**: Age, Income, Spending Score, Purchase Frequency, Order Value, Customer Years\n",
    "- **Accuracy**: High confidence predictions with distance-based scoring\n",
    "\n",
    "### üí° Business Applications:\n",
    "- **Marketing Campaigns**: Target specific customer segments\n",
    "- **Product Recommendations**: Personalize offerings per cluster\n",
    "- **Pricing Strategies**: Optimize pricing for different segments  \n",
    "- **Customer Retention**: Identify at-risk customer groups\n",
    "- **Resource Allocation**: Focus efforts on high-value segments\n",
    "\n",
    "Your model is production-ready! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export segmented customers to CSV\n",
    "df_export = df.copy()\n",
    "df_export['Cluster_Interpretation'] = df_export['Cluster'].apply(interpret_cluster)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = 'segmented_customers.csv'\n",
    "if IN_COLAB:\n",
    "    csv_path = f'/content/{csv_filename}'\n",
    "else:\n",
    "    csv_path = csv_filename\n",
    "\n",
    "df_export.to_csv(csv_path, index=False)\n",
    "print(f\"Segmented customers exported to '{csv_path}'\")\n",
    "\n",
    "# Download file in Google Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Downloading CSV file...\")\n",
    "    files.download(csv_path)\n",
    "    print(\"‚úÖ Download started! Check your browser's download folder.\")\n",
    "else:\n",
    "    print(f\"üìÅ File saved locally: {csv_path}\")\n",
    "\n",
    "# Business Insights and Recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in sorted(df['Cluster'].unique()):\n",
    "    cluster_data = df[df['Cluster'] == cluster_id]\n",
    "    count = len(cluster_data)\n",
    "    percentage = (count / len(df)) * 100\n",
    "    \n",
    "    avg_income = cluster_data['Annual_Income'].mean()\n",
    "    avg_spending = cluster_data['Spending_Score'].mean()\n",
    "    avg_frequency = cluster_data['Purchase_Frequency'].mean()\n",
    "    avg_order_value = cluster_data['Average_Order_Value'].mean()\n",
    "    \n",
    "    print(f\"\\nüéØ CLUSTER {cluster_id} - {interpret_cluster(cluster_id)}\")\n",
    "    print(f\"   Size: {count} customers ({percentage:.1f}% of total)\")\n",
    "    print(f\"   Key Metrics:\")\n",
    "    print(f\"   - Average Income: ${avg_income:,.0f}\")\n",
    "    print(f\"   - Spending Score: {avg_spending:.1f}/100\")\n",
    "    print(f\"   - Purchase Frequency: {avg_frequency:.1f}\")\n",
    "    print(f\"   - Average Order Value: ${avg_order_value:.2f}\")\n",
    "    \n",
    "    # Generate recommendations based on cluster characteristics\n",
    "    if avg_income > df['Annual_Income'].mean() and avg_spending > df['Spending_Score'].mean():\n",
    "        recommendations = [\n",
    "            \"Offer premium products and exclusive services\",\n",
    "            \"Implement VIP loyalty programs\",\n",
    "            \"Focus on personalized marketing campaigns\"\n",
    "        ]\n",
    "    elif avg_frequency > df['Purchase_Frequency'].mean():\n",
    "        recommendations = [\n",
    "            \"Reward frequent purchase behavior\",\n",
    "            \"Introduce subscription-based services\",\n",
    "            \"Cross-sell complementary products\"\n",
    "        ]\n",
    "    elif avg_spending < df['Spending_Score'].mean():\n",
    "        recommendations = [\n",
    "            \"Offer discounts and promotions\",\n",
    "            \"Introduce budget-friendly product lines\",\n",
    "            \"Focus on value-based marketing\"\n",
    "        ]\n",
    "    else:\n",
    "        recommendations = [\n",
    "            \"Implement retention strategies\",\n",
    "            \"Personalize product recommendations\",\n",
    "            \"Monitor for upselling opportunities\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"   üìù Recommendations:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"      ‚Ä¢ {rec}\")\n",
    "\n",
    "print(f\"\\nüöÄ OVERALL MODEL PERFORMANCE:\")\n",
    "print(f\"   - Number of Clusters: {optimal_k}\")\n",
    "print(f\"   - Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}\")\n",
    "print(f\"   - Total Customers Segmented: {len(df):,}\")\n",
    "print(f\"   - Features Used: {', '.join(features_for_clustering)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   All deliverables have been generated:\")\n",
    "print(f\"   - Customer segmentation model trained and saved\")\n",
    "print(f\"   - Clusters visualized using PCA and t-SNE\")\n",
    "print(f\"   - Business insights and recommendations provided\")\n",
    "print(f\"   - Segmented customer data exported to CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b27f2",
   "metadata": {},
   "source": [
    "## üéØ Google Colab Usage Guide\n",
    "\n",
    "### How to Run This Notebook in Google Colab:\n",
    "\n",
    "1. **Open in Colab**: \n",
    "   - Upload this `.ipynb` file to Google Colab\n",
    "   - Or copy the content and paste it into a new Colab notebook\n",
    "\n",
    "2. **Run All Cells**:\n",
    "   ```\n",
    "   Runtime ‚Üí Run all\n",
    "   ```\n",
    "   - This will install packages and execute the entire analysis\n",
    "\n",
    "3. **Run Individual Cells**:\n",
    "   - Click on a cell and press `Shift + Enter`\n",
    "   - Or click the play button on the left of each cell\n",
    "\n",
    "4. **Download Results**:\n",
    "   - The segmented customer CSV will automatically download\n",
    "   - Model files are saved in `/content/models/`\n",
    "\n",
    "### üí° Colab Tips:\n",
    "- **GPU Acceleration**: Go to `Runtime ‚Üí Change runtime type ‚Üí GPU` for faster processing\n",
    "- **File Upload**: Use the file upload widget to upload your own customer dataset\n",
    "- **Save Work**: Your notebook will be saved to Google Drive automatically\n",
    "- **Share**: Use the \"Share\" button to collaborate with others\n",
    "\n",
    "### üîß Troubleshooting:\n",
    "- **Package Issues**: Re-run the installation cell if you encounter import errors\n",
    "- **Memory Issues**: Use smaller dataset or restart runtime (`Runtime ‚Üí Restart runtime`)\n",
    "- **Timeout**: For large datasets, consider processing in smaller batches\n",
    "\n",
    "### üìä Expected Outputs:\n",
    "- Data visualizations and plots\n",
    "- Cluster analysis results\n",
    "- PCA and t-SNE visualizations\n",
    "- Downloadable CSV with customer segments\n",
    "- Saved ML models for future predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
